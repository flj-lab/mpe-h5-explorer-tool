Leveraging IronPython 2.7 and Parquet.Net for High-Performance Data Export in MTS Multipurpose Elite1. Understanding the Challenge: Large Data Export in MTS Multipurpose Elite (MPE)MTS Multipurpose Elite (MPE) users frequently encounter limitations when attempting to export exceptionally large datasets, particularly those measuring in the tens of gigabytes. The standard File > Export > Raw Data tool within MPE often becomes unresponsive or crashes under such conditions, indicating a significant memory constraint on systems with limited RAM, such as the 8GB available on the user's machine [User Query]. This issue underscores the need for a more robust and memory-efficient data export mechanism.Traditional text-based export formats, such as Comma Separated Values (CSV), are inherently inefficient for handling vast quantities of numerical data. While simple to parse, CSV files are row-oriented, leading to larger file sizes and slower input/output (I/O) operations when only specific columns are required for analysis. Furthermore, they lack advanced features like internal compression and schema enforcement, which are crucial for managing big data effectively. The user's current predicament highlights the limitations of such formats when confronted with the scale of modern test data.In contrast, Apache Parquet emerges as a highly suitable alternative for large-scale analytical data storage. Parquet is an open-source, columnar storage format designed for optimal performance with big data workloads.1 Unlike row-based formats, Parquet organizes data by columns, which significantly enhances compression ratios and query performance. This columnar structure allows for "predicate pushdown," where data filtering occurs early in the processing pipeline, minimizing the amount of data that needs to be read and transferred.1 This translates directly into reduced storage costs and faster analytical queries, making it an ideal choice for the user's scenario.2. IronPython 2.7 and.NET Interoperability in MPEMTS Multipurpose Elite provides a powerful scripting environment through its integration of IronPython 2.7, enabling users to interact directly with the underlying.NET framework.3 This interoperability is pivotal, as it allows for the development of custom solutions that extend beyond the capabilities of MPE's built-in tools. IronPython, as a.NET implementation of the Python language, provides seamless access to.NET assemblies, types, and methods via its clr module.5 This means that a Python script running within MPE can directly instantiate.NET objects, call their methods, and manipulate.NET data structures, bridging the gap between the scripting layer and the core application framework.The choice of IronPython 2.7 in MPE presents specific considerations regarding external library compatibility. While standard Python (CPython) offers popular libraries for Parquet handling like pyarrow and fastparquet, these libraries are typically built for Python 3.x and often rely on underlying C++ implementations or specific CPython features.7 For instance, pyarrow requires Python 3.9 or higher, and fastparquet also mandates Python 3.9 or newer versions.7 Such dependencies render them incompatible with IronPython 2.7. This incompatibility necessitates a different approach: leveraging a Parquet library developed natively for the.NET ecosystem, which can be seamlessly integrated and utilized through IronPython's CLR interoperability.3. Selecting an Optimal Parquet Writing Library for.NETGiven the requirement for a.NET-compatible Parquet writing solution within the IronPython 2.7 environment, Parquet.Net emerges as a highly suitable and performant choice. This library is specifically designed for the.NET world, offering a fully managed, pure.NET implementation for reading and writing Apache Parquet files.11 This design philosophy means it avoids the complexities and potential performance overheads associated with wrappers around non-.NET codebases.The performance characteristics of Parquet.Net are particularly noteworthy for handling large datasets. The library claims to be exceptionally fast, often surpassing the performance of Python and Java implementations, and in some cases, even native C++ solutions.11 This performance advantage is a critical factor when dealing with tens of gigabytes of data on a system with limited memory. By opting for a.NET-native library, the solution not only achieves compatibility but also potentially provides a superior and more efficient mechanism for large-scale data processing within the MPE environment. The native speed translates directly into faster export times, reduced CPU load, and a more responsive system during the data export process, transforming the solution from a mere workaround into a significant optimization. The library's focus on being.NET native ensures it is designed to utilize the.NET framework effectively, providing a simple and intuitive API for.NET developers.114. Implementing Parquet Export with IronPython 2.7 and Parquet.NetThe implementation of Parquet export within MPE using IronPython 2.7 and Parquet.Net involves several key steps, including setting up the library, adapting the existing data export function, and carefully managing data types and unit conversions.4.1 Setting up Parquet.Net in MPE/IronPythonTo integrate Parquet.Net with IronPython in MPE, the primary step is to obtain the Parquet.Net.dll assembly. This can typically be acquired by downloading the NuGet package and extracting the DLL file, rather than attempting to use package managers like pip or dotnet add package directly within the MPE environment. Once obtained, the Parquet.Net.dll file should be placed in a stable, easily accessible directory on the MPE machine, such as C:\MTS_Parquet_Libs. A dedicated folder is recommended for better organization and simplified management of libraries.Within the IronPython script, the clr module is used to load the Parquet.Net assembly. The most robust method for loading an assembly from a specific file path is clr.AddReferenceToFileAndPath(). This ensures that the correct DLL is referenced regardless of the MPE application's base directory or the Global Assembly Cache (GAC).5 It is also good practice to add the directory containing the DLL to sys.path, although AddReferenceToFileAndPath directly uses the full path. Error handling should be incorporated to gracefully manage scenarios where the assembly cannot be found or loaded, providing informative messages to the user.Pythonimport clr
import sys
import os
from System import Array, Double, String, Int32
from System.IO import FileStream, FileMode, FileAccess

# --- Parquet.Net Imports ---
# Define the absolute path where Parquet.Net.dll is located.
# The user MUST place the DLL at this specified path.
parquet_net_dll_path = r"C:\MTS_Parquet_Libs\Parquet.Net.dll" # Example path, user to confirm

# Add the directory containing the DLL to sys.path.
# This is good practice and can sometimes help with resolving dependencies.
sys.path.append(os.path.dirname(parquet_net_dll_path))

# Attempt to add reference to the Parquet.Net assembly.
# Wrap in try-except for robust error handling during deployment.
try:
    clr.AddReferenceToFileAndPath(parquet_net_dll_path)
    # Import necessary namespaces and classes from Parquet.Net
    from Parquet import ParquetWriter, ParquetConvert, ParquetOptions
    from Parquet.Data import Field, DataType, DataColumn, Schema
    # from Parquet.Compression import CompressionMethod # Uncomment if you want to specify compression
except Exception as e:
    # Print an informative error message if the assembly cannot be loaded.
    # In a production MPE environment, this might need to be logged to an MPE-specific log file
    # or displayed in a user-friendly manner within the MPE interface.
    print("Error: Could not load Parquet.Net assembly or namespaces.")
    print("Please ensure 'Parquet.Net.dll' is located at:", parquet_net_dll_path)
    print("Detailed error:", e)
    raise # Re-raise the exception to halt the script if a critical dependency is missing.
The necessity of manually placing DLLs and using clr.AddReferenceToFileAndPath underscores a key practical difference when working with embedded IronPython compared to a standard CPython environment that relies on pip for dependency management. This approach demands careful deployment planning and clear, step-by-step instructions for the user, as it is a common point of failure for those unfamiliar with.NET assembly loading mechanisms. Any future updates to Parquet.Net would also require manual replacement of the DLL.4.2 Adapting the SaveTestData() Function for Parquet StreamingThe fundamental architectural shift for handling large datasets is to transition from loading the entire dataset into memory and then writing it, to processing data in smaller, manageable chunks, known as "row groups" in Parquet. This approach significantly reduces the peak memory footprint, which is crucial for systems with limited RAM.The existing SaveTestData() function in MPE retrieves data arrays using calls such as RunningTimeArray.Value.ValueArray and LfLongPositionArray.Value.ValueArray.3 These calls return System.Double arrays, which are native.NET array types.3 While this method still materializes the entire dataset into MPE's process memory initially, the subsequent strategy involves iterating over sub-sections (chunks) of these large System.Double arrays to feed them to Parquet.Net's row group writer, thereby minimizing temporary Python list creation during the writing phase.Before any data can be written to a Parquet file, a schema must be explicitly defined. This schema informs Parquet.Net about the structure, names, and data types of each column. For numerical data acquired from MTS, DataType.Double is the appropriate choice, as IronPython's float type maps directly to the C# double type.13 The schema is constructed using Field objects, which are then aggregated into a Schema object from the Parquet.Data namespace.Python# Define schema fields for each column
# Use DataType.Double for numerical data from MTS
time_field = Field("Time", DataType.Double)
position_field = Field("LfLongPosition", DataType.Double)
# Add more fields for any other channels collected from MPE
# For example: force_field = Field("Force", DataType.Double)

# Create the schema object with all defined fields
schema = Schema(time_field, position_field) # Add all defined fields here
For efficient data streaming and chunking, a chunk_size variable is defined. This value determines the number of rows processed and written per row group. It is a critical parameter for balancing memory usage and write efficiency; larger chunks can lead to better compression and fewer I/O operations but require more temporary memory, while smaller chunks reduce memory consumption but increase overhead. A recommended starting point for chunk_size is typically between 100,000 and 1,000,000 rows.The process involves iterating through the c1_time_array and c2_position_array in increments of chunk_size. For each chunk, the relevant data segment is extracted, and new System.Double arrays are created using Array.CreateInstance() to hold the chunked data.14 These chunk-sized arrays are then used to create DataColumn objects, which Parquet.Net is designed to accept. The Parquet file is opened using a FileStream, and a ParquetWriter is initialized with this stream and the defined schema. Inside the chunking loop, writer.CreateRowGroup() is called to initiate a new data partition, and row_group_writer.WriteColumn() is invoked for each prepared DataColumn.A crucial consideration here is that while the proposed solution effectively chunks the writing of data, the initial RunningTimeArray.Value.ValueArray call in MPE still loads the entire dataset into the application's process memory. If this initial data materialization itself exceeds the 8GB RAM limit, the system might still encounter memory issues before the Parquet writing process can even begin. This means the current solution primarily optimizes the writing process from an already-loaded array rather than achieving true "streaming from source." If the problem persists, further investigation into MPE's internal APIs for partial data retrieval or batch acquisition directly from the hardware would be necessary to avoid the initial large memory allocation.Unit conversions and precision control are vital. The original SaveTestData() function explicitly applies unit conversions, such as multiplying LfLongPosition data by 1000 to convert from meters (MPE system unit) to millimeters (desired variable unit).3 This conversion logic must be applied to the numerical values before they are passed to Parquet.Net for writing, ensuring that the data is stored in the correct physical units. When using Parquet.Net with DataType.Double, precision is handled by the underlying floating-point representation. Unlike CSV export, which relies on string formatting (e.g., %.3f) for display precision, Parquet stores the raw binary Double values, preserving maximum numerical precision.13 This is generally beneficial for scientific and engineering data, as it maintains the highest possible accuracy. However, it also means the user must be diligent about applying all necessary unit transformations before the data is written, as Parquet will store the exact numerical value provided.The following code provides a modified SaveTestData() function incorporating Parquet.Net for chunked data export:Pythondef SaveTestData():
    # Get file name and directory from MPE variables [3]
    filename = FileNametoSave.Value.ValueAsString
    dir_path = DirectoryforSavefile.Value.ValueAsString
    
    # Construct the full output path, changing the extension to.parquet
    # Ensure the directory exists
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    full_path = os.path.join(dir_path, filename.replace(".dat", ".parquet"))

    # Read Data Arrays from Memory (assuming these are System.Double from MPE)
    # IMPORTANT NOTE: This step still loads the entire dataset into MPE's process memory
    # if ValueArray materializes it. For truly massive datasets (tens of GBs on 8GB RAM),
    # this initial call might still be a bottleneck.
    c1_time_array = RunningTimeArray.Value.ValueArray # Expected System.Double
    c2_position_array = LfLongPositionArray.Value.ValueArray # Expected System.Double

    datalength = len(c1_time_array)
    if datalength == 0:
        print("No data to save. Exiting SaveTestData.")
        return

    # Get the initial time value for zero-offset [3]
    time_init = c1_time_array

    # --- Define Parquet Schema ---
    # Define schema fields for each column. Use DataType.Double for numerical data.
    schema = Schema(
        Field("Time", DataType.Double),
        Field("LfLongPosition", DataType.Double)
        # Add more Field definitions here for additional channels
        # e.g., Field("Channel3Name", DataType.Double), etc.
    )

    # --- Parquet Writing Options (Optional) ---
    # Parquet.Net defaults to Snappy compression, which is a good balance of speed and compression.
    # You can customize options if needed.
    # options = ParquetOptions()
    # options.CompressionMethod = CompressionMethod.Gzip # Example: change compression
    # options.PageSize = 8 * 1024 * 1024 # Example: 8MB data page size

    # --- Open Parquet File Stream and Initialize Writer ---
    # FileMode.Create will create a new file or overwrite an existing one.
    # Parquet files are typically written once, not appended to incrementally.
    file_stream = None # Initialize to None for finally block
    writer = None # Initialize to None for finally block
    try:
        file_stream = FileStream(full_path, FileMode.Create, FileAccess.Write)
        # Initialize ParquetWriter with the file stream and schema
        writer = ParquetWriter.Create(file_stream, schema) # Pass options if defined: , options

        # --- Stream Data in Chunks (Row Groups) ---
        # Define a chunk size. This determines how many rows are processed and written per row group.
        # Experiment with this value for optimal performance vs. memory usage.
        # Typical values range from 100,000 to 1,000,000 rows per chunk for large datasets.
        chunk_size = 500000 # Example: 500,000 rows per chunk

        print("Starting Parquet export. Total data points:", datalength)

        for i in range(0, datalength, chunk_size):
            end_index = min(i + chunk_size, datalength)
            current_chunk_length = end_index - i

            # Create.NET System.Array for each column for the current chunk.
            # This involves copying data from the original large System.Double arrays
            # into smaller chunk-sized System.Double arrays.
            chunk_time_data = Array.CreateInstance(Double, current_chunk_length)
            chunk_position_data = Array.CreateInstance(Double, current_chunk_length)
            # Add more chunk arrays for additional channels

            for j in range(current_chunk_length):
                original_idx = i + j
                # Apply time zero-offset
                chunk_time_data.SetValue(c1_time_array[original_idx] - time_init, j)

                # Apply unit conversion for LfLongPosition [3]
                # MPE system unit for LfLongPosition might be 'm', but variable is 'mm'.
                # Multiply by 1000 to convert meters to millimeters before saving.
                chunk_position_data.SetValue(c2_position_array[original_idx] * 1000, j)
                # Apply conversions for other channels as needed

            # Create DataColumn objects for the current chunk.
            # DataColumn.Create takes a Field (from schema) and a System.Array of data.
            time_column = DataColumn(time_field, chunk_time_data)
            position_column = DataColumn(position_field, chunk_position_data)
            # Add more DataColumn objects for additional channels

            # Create a new row group and write columns to it.
            row_group_writer = writer.CreateRowGroup()
            row_group_writer.WriteColumn(time_column)
            row_group_writer.WriteColumn(position_column)
            # Write more columns as needed: row_group_writer.WriteColumn(channel3_column)

            print(f"  Written chunk {i//chunk_size + 1} of {current_chunk_length} rows.")

        print("All data chunks written.")

    except Exception as ex:
        print("An error occurred during Parquet export:", ex)
        # Log the full exception details for debugging
        # In MPE, you might need to use a specific logging mechanism if available.
        raise # Re-raise the exception to indicate failure in MPE.
    finally:
        # Ensure writer and file stream are closed, even if errors occur.
        if writer:
            writer.Dispose() # Or writer.Close() if Dispose is not available/preferred
        if file_stream:
            file_stream.Close()

    print("Data successfully exported to Parquet:", full_path)

# To run this function within MPE, you would typically assign it to a "Function"
# and call it from a "Program Action" as demonstrated in [3]:
# dummy = SaveTestData()
Table 1: Mapping MTS Data Types to Parquet.Net Schema TypesTo ensure data integrity and compatibility when migrating from MTS TestSuite variables to Parquet, it is essential to correctly map the data types. The following table provides guidance on mapping common MTS variable types to their corresponding IronPython/CLR types and the recommended Parquet.Net DataType for schema definition.MTS TestSuite Variable TypeTypical IronPython/CLR TypeRecommended Parquet.Net DataTypeNotesNumber ArraySystem.DoubleDataType.DoubleFor continuous numerical data (e.g., time series, position, force). Unit conversions may apply.NumberSystem.DoubleDataType.DoubleFor single numerical values.TextSystem.StringDataType.StringFor textual or categorical data.BooleanSystem.BooleanDataType.BooleanFor true/false flags.Date/TimeSystem.DateTimeDataType.TimestampFor date and time information. Ensure correct time resolution and timezone handling.IntegerSystem.Int32 or System.Int64DataType.Int32 or DataType.Int64For discrete integer values. Choose based on value range.5. Deployment and Performance OptimizationEffective deployment and optimization are critical to fully realize the benefits of using Parquet.Net for large data exports in MPE.5.1 Best Practices for Deploying Parquet.Net DLLsFor seamless operation, the Parquet.Net.dll file, along with any other necessary.NET assemblies, should be placed in a centralized, stable directory on the MPE machine (e.g., C:\MTS_Parquet_Libs). This approach simplifies management, referencing, and future updates, preventing file scattering and potential conflicts. It is imperative to ensure that the user account running MPE, and consequently the IronPython script, possesses the necessary read and execute permissions for these DLL files, as well as write permissions for the designated output directory where the Parquet files will be saved. For environments with multiple MPE installations or scripts, establishing a clear versioning strategy for deployed DLLs is advisable to maintain consistency and prevent unexpected behavior.While Parquet.Net prides itself on having "zero dependencies" 11, this typically refers to external, third-party libraries. It is important to note that it might still implicitly rely on specific versions of core.NET Framework assemblies that may or may not be present or fully compatible on older or highly customized MPE installations. Should the user encounter FileNotFoundException errors for seemingly unrelated DLLs (e.g., System.Collections.Immutable.dll), this would indicate a missing transitive dependency. In such cases, these additional required DLLs would also need to be deployed alongside Parquet.Net.dll. This readiness for potential implicit dependencies is a crucial troubleshooting consideration in real-world embedded environments.5.2 Strategies for Maximizing Memory Efficiency and Write PerformanceOptimizing the chunk_size variable within the SaveTestData() function is a primary lever for balancing memory consumption and write performance. Smaller chunks consume less memory at any given time, thereby reducing the risk of exceeding the 8GB RAM limit. However, they introduce more overhead due to increased calls to CreateRowGroup and more frequent disk flushes, which can potentially slow down the overall export process. Conversely, larger chunks can lead to better compression ratios and fewer I/O operations, potentially resulting in faster overall writes, but they demand more temporary memory per chunk. It is recommended to start with a moderate chunk_size (e.g., 100,000 to 500,000 rows) and systematically experiment to identify the optimal value for the specific MPE system and the characteristics of the data being exported.When defining the Parquet schema, utilizing DataType.Double for floating-point numbers is crucial for preserving the full precision of the original MTS data.13 This is generally desirable for engineering and scientific measurements, where data accuracy is paramount. Converting to lower-precision types should be avoided unless there is a compelling reason for storage savings, as it can lead to irreversible data loss.Parquet.Net defaults to Snappy compression, which offers an excellent balance between high compression speed and a good compression ratio.16 This is often the most suitable choice for performance-sensitive applications. For scenarios where maximum compression is the overriding priority, other codecs like Gzip or Zstandard (if supported by Parquet.Net and its dependencies) could be explored.10 However, these typically incur higher CPU overhead during both write and read operations. For memory-constrained systems, faster compression and decompression can indirectly contribute to memory efficiency by reducing the amount of data that needs to be buffered in memory for I/O operations.The most substantial performance and memory efficiency gains, beyond the Parquet.Net chunking, would stem from minimizing data copies at the source. As previously noted, the RunningTimeArray.Value.ValueArray call in MPE currently loads the entire dataset into memory. If MPE's API were to offer a mechanism for retrieving data in smaller, iterable segments (e.g., a DataReader-like pattern in.NET or an IEnumerable that fetches data on demand), this would represent a true streaming solution. Such an approach would eliminate the initial large memory allocation entirely, providing the ultimate mitigation for handling "tens of gigabytes" of data on an 8GB RAM system. The current approach of creating Array.CreateInstance for each chunk, while efficient for writing, still involves copying data from the large source arrays. If Parquet.Net's API or IronPython's interoperability could allow direct iteration over the source System.Double without explicit chunk-by-chunk copying, further memory optimization could be achieved.6. Conclusions and RecommendationsThe analysis demonstrates that leveraging IronPython 2.7 with the Parquet.Net library provides a robust and performant solution for exporting large datasets from MTS Multipurpose Elite. This approach effectively addresses the limitations of MPE's default export tool, particularly its susceptibility to memory-related hangs when processing tens of gigabytes of data on systems with limited RAM.The adoption of Apache Parquet offers significant advantages due to its columnar storage format, which facilitates efficient data compression and faster analytical querying compared to traditional row-oriented formats like CSV.1 The selection of Parquet.Net is particularly advantageous given its pure.NET implementation and documented high performance, which can surpass other language implementations.11 This makes it not merely a compatible choice but a strategic one for optimizing data export within the MPE environment.Actionable Recommendations:
Implement the Provided Script: The user should implement the modified SaveTestData() IronPython script, ensuring that the Parquet.Net.dll assembly is correctly deployed to a stable, accessible directory (e.g., C:\MTS_Parquet_Libs) and properly referenced within the script using clr.AddReferenceToFileAndPath().
Apply Unit Conversions Diligently: All necessary unit conversions (e.g., meters to millimeters) must be applied to the numerical data before it is passed to Parquet.Net for writing. While Parquet preserves the full precision of System.Double values, the numerical transformation itself is the user's responsibility.
Optimize Chunk Size: Experimentation with the chunk_size variable (e.g., starting with 100,000 to 500,000 rows) is recommended to find the optimal balance between memory consumption and write performance for the specific MPE system and data characteristics.
Monitor Initial Data Load: The user should closely monitor memory consumption during the initial data acquisition step (e.g., RunningTimeArray.Value.ValueArray). If memory issues persist at this stage, it indicates that MPE's internal data retrieval mechanism is the primary bottleneck. In such a scenario, further investigation into MPE's documentation or support for alternative, truly streaming data access APIs would be necessary to avoid loading the entire dataset into memory at once.
Leverage Parquet for Downstream Analysis: Beyond solving the export problem, the adoption of Parquet files will significantly benefit subsequent data analysis workflows due to their inherent efficiency in storage, compression, and query performance.


